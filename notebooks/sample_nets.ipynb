{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_words(arrrrrr):\n",
    "    l = 0 \n",
    "    for i in range(25000):\n",
    "        temp = len(arrrrrr[i])\n",
    "        if temp>l:\n",
    "            l = temp\n",
    "        else:\n",
    "            continue\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt('../datasets/imdb_xtrain.csv', x_train, delimiter=',')\n",
    "savetxt('../datasets/imdb_xtest.csv', x_test, delimiter=',')\n",
    "savetxt('../datasets/imdb_ytrain.csv', y_train, delimiter=',')\n",
    "savetxt('../datasets/imdb_ytest.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainsample = pd.read_csv('../datasets/imdb_xtrain.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 400-Word Run CNN (4 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Default Parameters \n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 4\n",
    "\n",
    "x_train_400 = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_400 = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model_1.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model_1.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model_1.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model_1.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model_1.add(Dense(hidden_dims))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model_1.add(Dense(1))\n",
    "model_1.add(Activation('sigmoid'))\n",
    "\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekuo651\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.4104 - accuracy: 0.7954 - val_loss: 0.2825 - val_accuracy: 0.8818\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 94s 4ms/step - loss: 0.2292 - accuracy: 0.9088 - val_loss: 0.2818 - val_accuracy: 0.8852\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.1651 - accuracy: 0.9385 - val_loss: 0.2766 - val_accuracy: 0.8904\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 96s 4ms/step - loss: 0.1150 - accuracy: 0.9586 - val_loss: 0.2845 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24f03c902e8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(x_train_400, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_400, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000-Word Run CNN (4 Epochs, and then another 4, and then another 4)\n",
    "\n",
    "Results weren't much better than the 400 word run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (25000, 1000)\n",
      "x_test (25000, 1000)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 50)          250000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 998, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Increasing maximum length of text to 1000 words, 2 epochs\n",
    "max_features = 5000\n",
    "maxlen = 1000\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 4\n",
    "\n",
    "\n",
    "x_train_1000 = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_1000 = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('x_train',x_train_1000.shape)\n",
    "print('x_test',x_test_1000.shape)\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model_2.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model_2.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model_2.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model_2.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model_2.add(Dense(hidden_dims))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model_2.add(Dense(1))\n",
    "model_2.add(Activation('sigmoid'))\n",
    "\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekuo651\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 239s 10ms/step - loss: 0.4076 - accuracy: 0.7960 - val_loss: 0.2722 - val_accuracy: 0.8886\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 242s 10ms/step - loss: 0.2293 - accuracy: 0.9080 - val_loss: 0.3187 - val_accuracy: 0.8595\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 240s 10ms/step - loss: 0.1596 - accuracy: 0.9384 - val_loss: 0.2718 - val_accuracy: 0.8936\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 243s 10ms/step - loss: 0.1128 - accuracy: 0.9592 - val_loss: 0.3164 - val_accuracy: 0.8871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24f04498128>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train_1000, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_1000, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 243s 10ms/step - loss: 0.0758 - accuracy: 0.9734 - val_loss: 0.3519 - val_accuracy: 0.8876\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 243s 10ms/step - loss: 0.0543 - accuracy: 0.9808 - val_loss: 0.3935 - val_accuracy: 0.8815\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 242s 10ms/step - loss: 0.0505 - accuracy: 0.9811 - val_loss: 0.3740 - val_accuracy: 0.8889\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 244s 10ms/step - loss: 0.0382 - accuracy: 0.9852 - val_loss: 0.4937 - val_accuracy: 0.8888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24f044c2fd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train_1000, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_1000, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 244s 10ms/step - loss: 0.0307 - accuracy: 0.9886 - val_loss: 0.4820 - val_accuracy: 0.8864\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 244s 10ms/step - loss: 0.0367 - accuracy: 0.9864 - val_loss: 0.4709 - val_accuracy: 0.8823\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 248s 10ms/step - loss: 0.0287 - accuracy: 0.9893 - val_loss: 0.4864 - val_accuracy: 0.8874\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.0275 - accuracy: 0.9898 - val_loss: 0.5151 - val_accuracy: 0.8872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24f044fe780>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train_1000, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_1000, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 400-Word Run CNN - Double Kernel Size\n",
    "Faster than 1000 word run. Achieves over .89 val_accuracy by 3rd epoch, which is slightly higher than default kernel size of 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 395, 250)          75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 388,251\n",
      "Trainable params: 388,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 6\n",
    "hidden_dims = 250\n",
    "epochs = 4\n",
    "\n",
    "model_3 = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model_3.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model_3.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model_3.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model_3.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model_3.add(Dense(hidden_dims))\n",
    "model_3.add(Dropout(0.2))\n",
    "model_3.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model_3.add(Dense(1))\n",
    "model_3.add(Activation('sigmoid'))\n",
    "\n",
    "model_3.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekuo651\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 118s 5ms/step - loss: 0.4134 - accuracy: 0.7926 - val_loss: 0.2725 - val_accuracy: 0.8885\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 120s 5ms/step - loss: 0.2190 - accuracy: 0.9145 - val_loss: 0.2764 - val_accuracy: 0.8846\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 121s 5ms/step - loss: 0.1501 - accuracy: 0.9432 - val_loss: 0.2660 - val_accuracy: 0.8923\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 120s 5ms/step - loss: 0.0999 - accuracy: 0.9650 - val_loss: 0.3206 - val_accuracy: 0.8910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24f0486fc88>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(x_train_400, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_400, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 117s 5ms/step - loss: 0.0615 - accuracy: 0.9792 - val_loss: 0.3657 - val_accuracy: 0.8882\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 120s 5ms/step - loss: 0.0451 - accuracy: 0.9858 - val_loss: 0.4242 - val_accuracy: 0.8854\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 121s 5ms/step - loss: 0.0319 - accuracy: 0.9884 - val_loss: 0.4736 - val_accuracy: 0.8863\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 121s 5ms/step - loss: 0.0378 - accuracy: 0.9864 - val_loss: 0.5255 - val_accuracy: 0.8826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24f080da588>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(x_train_400, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_400, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80-word LSTM \n",
    "Max `val_accuracy` of ~.82, peaking between epoch 3-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=80\n",
    "x_train_80 = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_80 = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(Embedding(max_features, 128))\n",
    "model_4.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_4.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 102s 4ms/step - loss: 0.3862 - accuracy: 0.8334 - val_loss: 0.3798 - val_accuracy: 0.8304\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 104s 4ms/step - loss: 0.3274 - accuracy: 0.8624 - val_loss: 0.3789 - val_accuracy: 0.8307\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 105s 4ms/step - loss: 0.2795 - accuracy: 0.8845 - val_loss: 0.4029 - val_accuracy: 0.8335\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 105s 4ms/step - loss: 0.2436 - accuracy: 0.9008 - val_loss: 0.4104 - val_accuracy: 0.8302\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 105s 4ms/step - loss: 0.2070 - accuracy: 0.9182 - val_loss: 0.4435 - val_accuracy: 0.8250\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.1818 - accuracy: 0.9297 - val_loss: 0.4880 - val_accuracy: 0.8356\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 113s 5ms/step - loss: 0.1493 - accuracy: 0.9430 - val_loss: 0.4978 - val_accuracy: 0.8229\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 108s 4ms/step - loss: 0.1285 - accuracy: 0.9517 - val_loss: 0.5602 - val_accuracy: 0.8260\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.1110 - accuracy: 0.9582 - val_loss: 0.5933 - val_accuracy: 0.8197\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.0862 - accuracy: 0.9699 - val_loss: 0.6843 - val_accuracy: 0.8130\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 109s 4ms/step - loss: 0.0718 - accuracy: 0.9740 - val_loss: 0.7653 - val_accuracy: 0.8218\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 108s 4ms/step - loss: 0.0611 - accuracy: 0.9794 - val_loss: 0.7618 - val_accuracy: 0.8192\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 111s 4ms/step - loss: 0.0536 - accuracy: 0.9814 - val_loss: 0.8224 - val_accuracy: 0.8203\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 108s 4ms/step - loss: 0.0486 - accuracy: 0.9836 - val_loss: 0.8079 - val_accuracy: 0.8173\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 107s 4ms/step - loss: 0.0353 - accuracy: 0.9885 - val_loss: 0.8973 - val_accuracy: 0.8152\n",
      "25000/25000 [==============================] - 21s 820us/step\n",
      "Test score: 0.8972654736280441\n",
      "Test accuracy: 0.8151599764823914\n"
     ]
    }
   ],
   "source": [
    "model_4.fit(x_train_80, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test_80, y_test))\n",
    "score, acc = model_4.evaluate(x_test_80, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80-Word LSTM (Increase dropout to .3, `relu` activation)\n",
    "Worse results than default of .2 dropout and sigmoid activation. Probably should not have tested both at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000,)\n",
      "x_test shape: (25000,)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 128)         640000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 771,713\n",
      "Trainable params: 771,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxlen=80\n",
    "x_train_80 = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_80 = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "model_5 = Sequential()\n",
    "model_5.add(Embedding(max_features, 128))\n",
    "model_5.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2))\n",
    "model_5.add(Dense(1, activation='relu'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_5.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekuo651\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.5981 - accuracy: 0.7018 - val_loss: 0.5070 - val_accuracy: 0.7717\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 112s 4ms/step - loss: 0.5189 - accuracy: 0.7911 - val_loss: 0.5208 - val_accuracy: 0.7722\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 110s 4ms/step - loss: 0.5006 - accuracy: 0.8073 - val_loss: 0.5470 - val_accuracy: 0.8069\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 108s 4ms/step - loss: 0.4111 - accuracy: 0.8412 - val_loss: 0.5038 - val_accuracy: 0.7998\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 107s 4ms/step - loss: 0.3842 - accuracy: 0.8571 - val_loss: 0.5535 - val_accuracy: 0.8121\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 107s 4ms/step - loss: 0.3580 - accuracy: 0.8646 - val_loss: 0.5113 - val_accuracy: 0.7976\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 107s 4ms/step - loss: 0.3244 - accuracy: 0.8823 - val_loss: 0.6349 - val_accuracy: 0.8035\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 105s 4ms/step - loss: 0.3041 - accuracy: 0.8940 - val_loss: 0.7659 - val_accuracy: 0.8150\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.3144 - accuracy: 0.8868 - val_loss: 0.6030 - val_accuracy: 0.8162\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 102s 4ms/step - loss: 0.2856 - accuracy: 0.8918 - val_loss: 0.9473 - val_accuracy: 0.7953\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 104s 4ms/step - loss: 0.2997 - accuracy: 0.8842 - val_loss: 0.8961 - val_accuracy: 0.7999\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.2575 - accuracy: 0.8982 - val_loss: 0.8283 - val_accuracy: 0.7961\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.2193 - accuracy: 0.8983 - val_loss: 1.0217 - val_accuracy: 0.7736\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.1994 - accuracy: 0.8862 - val_loss: 1.1132 - val_accuracy: 0.7568\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 104s 4ms/step - loss: 0.2107 - accuracy: 0.8522 - val_loss: 1.0566 - val_accuracy: 0.7817\n",
      "25000/25000 [==============================] - 20s 809us/step\n",
      "Test score: 1.056603301486969\n",
      "Test accuracy: 0.781719982624054\n"
     ]
    }
   ],
   "source": [
    "model_5.fit(x_train_80, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test_80, y_test))\n",
    "score, acc = model_5.evaluate(x_test_80, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 120-Word LSTM\n",
    "Increase in `val_accuracy` from ~.82 to ~.86. Reaches max by epoch 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 120)\n",
      "x_test shape: (25000, 120)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 128)         640000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 771,713\n",
      "Trainable params: 771,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekuo651\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 153s 6ms/step - loss: 0.4694 - accuracy: 0.7799 - val_loss: 0.4436 - val_accuracy: 0.8219\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 154s 6ms/step - loss: 0.3690 - accuracy: 0.8426 - val_loss: 0.4048 - val_accuracy: 0.8209\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 154s 6ms/step - loss: 0.3288 - accuracy: 0.8628 - val_loss: 0.3365 - val_accuracy: 0.8578\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 158s 6ms/step - loss: 0.2624 - accuracy: 0.8931 - val_loss: 0.3518 - val_accuracy: 0.8614\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 156s 6ms/step - loss: 0.2207 - accuracy: 0.9118 - val_loss: 0.3626 - val_accuracy: 0.8506\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 157s 6ms/step - loss: 0.1796 - accuracy: 0.9316 - val_loss: 0.3874 - val_accuracy: 0.8576\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 155s 6ms/step - loss: 0.1497 - accuracy: 0.9433 - val_loss: 0.4127 - val_accuracy: 0.8572\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 155s 6ms/step - loss: 0.1262 - accuracy: 0.9546 - val_loss: 0.4634 - val_accuracy: 0.8494\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 155s 6ms/step - loss: 0.1017 - accuracy: 0.9640 - val_loss: 0.4446 - val_accuracy: 0.8488\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 155s 6ms/step - loss: 0.0871 - accuracy: 0.9696 - val_loss: 0.5397 - val_accuracy: 0.8507\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 155s 6ms/step - loss: 0.0687 - accuracy: 0.9760 - val_loss: 0.5928 - val_accuracy: 0.8504\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 155s 6ms/step - loss: 0.0544 - accuracy: 0.9821 - val_loss: 0.6370 - val_accuracy: 0.8433\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 155s 6ms/step - loss: 0.0475 - accuracy: 0.9840 - val_loss: 0.6913 - val_accuracy: 0.8440\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 156s 6ms/step - loss: 0.0397 - accuracy: 0.9866 - val_loss: 0.7178 - val_accuracy: 0.8478\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 155s 6ms/step - loss: 0.0338 - accuracy: 0.9894 - val_loss: 0.7769 - val_accuracy: 0.8425\n",
      "25000/25000 [==============================] - 31s 1ms/step\n",
      "Test score: 0.7769462085628509\n",
      "Test accuracy: 0.8424800038337708\n"
     ]
    }
   ],
   "source": [
    "maxlen=120\n",
    "x_train_120 = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_120 = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train_120.shape)\n",
    "print('x_test shape:', x_test_120.shape)\n",
    "\n",
    "model_6 = Sequential()\n",
    "model_6.add(Embedding(max_features, 128))\n",
    "model_6.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_6.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_6.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_6.summary()\n",
    "\n",
    "model_6.fit(x_train_120, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test_120, y_test))\n",
    "score, acc = model_6.evaluate(x_test_120, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 240-Word LSTM\n",
    "Reaches max `val_accuracy` by epoch 10 with score .8723. Slight improvement on the 120-word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 240)\n",
      "x_test shape: (25000, 240)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekuo651\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 302s 12ms/step - loss: 0.5036 - accuracy: 0.7520 - val_loss: 0.3692 - val_accuracy: 0.8444\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 305s 12ms/step - loss: 0.4324 - accuracy: 0.8051 - val_loss: 0.4149 - val_accuracy: 0.8198\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 304s 12ms/step - loss: 0.3549 - accuracy: 0.8506 - val_loss: 0.3697 - val_accuracy: 0.8462\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 304s 12ms/step - loss: 0.3022 - accuracy: 0.8770 - val_loss: 0.3393 - val_accuracy: 0.8590\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 303s 12ms/step - loss: 0.3073 - accuracy: 0.8725 - val_loss: 0.3611 - val_accuracy: 0.8485\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 305s 12ms/step - loss: 0.2697 - accuracy: 0.8916 - val_loss: 0.3514 - val_accuracy: 0.8599\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 310s 12ms/step - loss: 0.2363 - accuracy: 0.9070 - val_loss: 0.3373 - val_accuracy: 0.8682\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 305s 12ms/step - loss: 0.1887 - accuracy: 0.9278 - val_loss: 0.3927 - val_accuracy: 0.8506\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 307s 12ms/step - loss: 0.1637 - accuracy: 0.9374 - val_loss: 0.3812 - val_accuracy: 0.8683\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 302s 12ms/step - loss: 0.1360 - accuracy: 0.9495 - val_loss: 0.3837 - val_accuracy: 0.8723\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 310s 12ms/step - loss: 0.1152 - accuracy: 0.9576 - val_loss: 0.4389 - val_accuracy: 0.8682\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 310s 12ms/step - loss: 0.1011 - accuracy: 0.9639 - val_loss: 0.4188 - val_accuracy: 0.8658\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 309s 12ms/step - loss: 0.0812 - accuracy: 0.9710 - val_loss: 0.4548 - val_accuracy: 0.8635\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 310s 12ms/step - loss: 0.0709 - accuracy: 0.9754 - val_loss: 0.4974 - val_accuracy: 0.8612\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 309s 12ms/step - loss: 0.0610 - accuracy: 0.9785 - val_loss: 0.5170 - val_accuracy: 0.8564\n",
      "25000/25000 [==============================] - 61s 2ms/step\n",
      "Test score: 0.516984115600586\n",
      "Test accuracy: 0.8563600182533264\n"
     ]
    }
   ],
   "source": [
    "maxlen=240\n",
    "x_train_240 = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_240 = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train_240.shape)\n",
    "print('x_test shape:', x_test_240.shape)\n",
    "\n",
    "model_7 = Sequential()\n",
    "model_7.add(Embedding(max_features, 128))\n",
    "model_7.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_7.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_7.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_7.fit(x_train_240, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test_240, y_test))\n",
    "score, acc = model_7.evaluate(x_test_240, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 400-Word LSTM \n",
    "Starts off with lower `val_accuracy` than previous runs. Hits ~.86 accuracy by epoch 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekuo651\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 527s 21ms/step - loss: 0.5076 - accuracy: 0.7532 - val_loss: 0.4911 - val_accuracy: 0.7598\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 522s 21ms/step - loss: 0.3862 - accuracy: 0.8365 - val_loss: 0.3748 - val_accuracy: 0.8379\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 524s 21ms/step - loss: 0.3335 - accuracy: 0.8628 - val_loss: 0.3395 - val_accuracy: 0.8612\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 513s 21ms/step - loss: 0.2918 - accuracy: 0.8802 - val_loss: 0.4047 - val_accuracy: 0.8353\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 535s 21ms/step - loss: 0.2612 - accuracy: 0.8971 - val_loss: 0.3328 - val_accuracy: 0.8641\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 531s 21ms/step - loss: 0.2299 - accuracy: 0.9081 - val_loss: 0.3924 - val_accuracy: 0.8253\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 531s 21ms/step - loss: 0.2108 - accuracy: 0.9157 - val_loss: 0.4005 - val_accuracy: 0.8101\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 531s 21ms/step - loss: 0.1793 - accuracy: 0.9300 - val_loss: 0.3371 - val_accuracy: 0.8757\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 534s 21ms/step - loss: 0.1463 - accuracy: 0.9441 - val_loss: 0.3591 - val_accuracy: 0.8648\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 535s 21ms/step - loss: 0.1298 - accuracy: 0.9518 - val_loss: 0.3574 - val_accuracy: 0.8709\n",
      "25000/25000 [==============================] - 102s 4ms/step\n",
      "Test score: 0.35736553396701815\n",
      "Test accuracy: 0.8708800077438354\n"
     ]
    }
   ],
   "source": [
    "maxlen=400\n",
    "x_train_400 = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_400 = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train_400.shape)\n",
    "print('x_test shape:', x_test_400.shape)\n",
    "\n",
    "model_8 = Sequential()\n",
    "model_8.add(Embedding(max_features, 128))\n",
    "model_8.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_8.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_8.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_8.fit(x_train_400, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test_400, y_test))\n",
    "score, acc = model_8.evaluate(x_test_400, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.fit(x_train_400, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test_400, y_test))\n",
    "score, acc = model_8.evaluate(x_test_400, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
